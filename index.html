<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Practical Machine Learning: Prediction Assignment Report</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>Practical Machine Learning: Prediction Assignment Report</h2>

<p>This report describes the analysis made for the course project of the Practical Machine Learning course on coursera. The analysis is divided in 3 steps:</p>

<ol>
<li>Data cleaning</li>
<li>Data and model exploratory analysis</li>
<li>Final prediction and conclusion</li>
</ol>

<h1>Data cleaning</h1>

<p>First step, reading the data:</p>

<pre><code class="R">training &lt;- read.csv(&quot;pml-training.csv&quot;)
testing &lt;- read.csv(&quot;pml-testing.csv&quot;)
</code></pre>

<p>After reading the data, it is readily apparent upon initial investigation that some columns have a lot of missing values.
Using the command below, the values are counted, and it is observed that columns either have all missing values or none:</p>

<pre><code class="R">lst &lt;- colnames(training[,-(1:2)])
count.na.training &lt;- sapply(lst,FUN=function(x,training){sum(is.na(training[,x]))},training)
</code></pre>

<p>And also checking the testing data:</p>

<pre><code>lst &lt;- colnames(testing[,-(1:2)])
count.na.testing &lt;- sapply(lst,FUN=function(x,testing){sum(is.na(testing[,x]))},testing)
</code></pre>

<p>As such, the next step is to remove the columns that contain missing values entirely, filtering 100 out of 160 total colums.
And since the testing dataset has more columns with all missing values, it is used as base to filter the training set:</p>

<pre><code class="R">boolTest &lt;- colSums(is.na(testing)) == 0
testing &lt;- testing[, boolTest]
training &lt;- training[, boolTest]
</code></pre>

<p>The first 7 columns are just identifiers (ID, name etc.) so they can also be removed, as they won&#39;t add to the analysis.
As an addendum, the seventh could be something relevant (&ldquo;num_window&rdquo;), as in some sort of time window where each exercise set is performed, but without some sort of documentation and upon further analysis (albeit shallow) it does not seem to hold much valuable information:</p>

<pre><code class="R">training &lt;- training[,-c(1:7)]
testing &lt;- testing[,-c(1:7)]
</code></pre>

<h1>Data and model exploratory analysis</h1>

<p>After the initial data cleaning, the next step is to evaluate how a few models perform on the data, before attempting some sort of feature engineering.</p>

<pre><code class="R">set.seed(111)
</code></pre>

<p>For this, it was used a holdout method, splitting the training data into 2 chunks, and using one to train, and one to evaluate:</p>

<pre><code class="R">inTrain &lt;- createDataPartition( y = training$classe, p = 0.6, list = FALSE )
training60part &lt;- training[inTrain,]
testing60part &lt;- training[-inTrain,]
</code></pre>

<p>Initially, fitting a stochastic gradient boosting (GBM) model with default parameters:</p>

<pre><code class="R">fitGBM &lt;- train( classe ~ . , data = training60part, method = &quot;gbm&quot; )
</code></pre>

<p>Predicting with the GBM:</p>

<pre><code class="R">predGBM &lt;- predict( fitGBM, testing60part )
</code></pre>

<p>The confusion matrix shows a ~96% accuracy, which seems pretty good for the initial run</p>

<pre><code class="R">confusionMatrix( predGBM, testing60part$classe )
</code></pre>

<p>Now fitting a random forest with default parameters to see which method performs better:</p>

<pre><code class="R">fitRF &lt;- train( classe ~ . , data = training60part, method = &quot;rf&quot;)
predRF &lt;- predict( fitRF, testing60part )
confusionMatrix( predRF, testing60part$classe )
</code></pre>

<p>The random forest achieves 98,75% accuracy</p>

<h1>Final prediction and conclusion</h1>

<p>Considering the already good performance of the random forest model, it seemed reasonable to not do any further exploratory analysis, and skip to a more robust estimation of the out-of-sample error.
As such, the next step is a 5-fold x 2 cross-validation using a random forest to narrow down the prediction of the out-of-sample error:</p>

<p>Setting a seed for reproducibility:</p>

<pre><code class="R">set.seed(1111)
</code></pre>

<p>Setting up the cross validation options:</p>

<pre><code class="R">`fitControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 5, repeats = 2)`
</code></pre>

<p>Fitting the model and predicting:</p>

<pre><code class="R">fitRF &lt;- train( classe ~ ., data = training60part, method = &quot;rf&quot;, trControl = fitControl )
fitRF$pred
</code></pre>

<p>The random forest with 5x2 cross validation indicates around 98,5% accuracy, so at this point it seems reasonable to estimate the out-of-sample error to be at around 1,5%</p>

</body>

</html>
